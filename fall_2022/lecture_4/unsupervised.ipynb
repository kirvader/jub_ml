{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6598e6d3",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966293f3",
   "metadata": {},
   "source": [
    "## 0 Recap on Vector Space and Metric \n",
    "\n",
    "**Vector space** ?\n",
    "\n",
    "\n",
    "\n",
    "**Metric** ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf9928",
   "metadata": {},
   "source": [
    "## 1 Clustering \n",
    "\n",
    "**Problem**:  \n",
    "\n",
    "Given finite dataset $\\{x_i\\}_{i=1}^N$ find mapping $h$: $X \\rightarrow \\{1..K\\}$,  \n",
    "where $K$ is the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3bc2ef",
   "metadata": {},
   "source": [
    "### 1.1 Clustering Metrics\n",
    "\n",
    "Let $c_k$ be the cluster centers.\n",
    "\n",
    "**Intracluster distance (unsupervised)**\n",
    "\n",
    "$$D_{in} = \\sum_{i=1}^N \\sum_{k=1}^K [h(x_i) = k] d(x_i, c_k) $$\n",
    "\n",
    "We want to minimize intracluster distance.\n",
    "\n",
    "**Intercluster distance (unsupervised)**\n",
    "\n",
    "$$D_{out} = \\sum_{i=1}^N \\sum_{j=1}^N [h(x_i) \\neq h(x_j)] d(x_i, x_j) $$\n",
    "\n",
    "We want to maximize intercluster distance.\n",
    "\n",
    "**Sihouette (supervised)**\n",
    "\n",
    "If you have some ground truth cluster assignments.  \n",
    "\n",
    "Let $a$ - mean distance between the sample and other objects in the same class\n",
    "$$a = \\frac 1 {|S_{t(x)}|} \\sum_{x_j \\in t(x)} d(x, x_j)$$\n",
    "\n",
    "Let $b$ - mean distance between the sample and the objects from the nearest cluster\n",
    "$$b = \\min_{k} \\frac 1 {|S_k|} \\sum_{x_j \\in S_k, x \\notin S_k} d(x, x_j)$$\n",
    "\n",
    "$$S = \\frac 1 N \\sum_{i=1}^N s(x_i) = \\frac 1 N \\sum_{i=1}^N \\frac {b(x_i) - a(x_i)} {\\max(b(x_i), a(x_i))} $$\n",
    "\n",
    "**F1-score (supervised)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8614e5c7",
   "metadata": {},
   "source": [
    "### 1.2 K-means\n",
    "\n",
    "**Objective**\n",
    "\n",
    "\n",
    "$$\\sum_{i=1}^N \\sum_{k=1}^K [x_i \\in S_k] d(x_i, c_k) \\rightarrow \\min_{S_k}$$\n",
    "\n",
    "We want assign clusters to minimize intercluster distance.\n",
    "\n",
    "**Algorithm sketch**\n",
    "\n",
    "1. Select initial cluster centers $c_k^{(0)}$  \n",
    "1. Assign objects to clusters $$h(x) = \\arg \\min_{k} d(x, c_k)$$  \n",
    "1. Update cluster centers $$c_k^{(t+1)} = \\frac 1 {|S_k|} \\sum_{x_i \\in S_k} x_i $$ \n",
    "1. Repeat steps 2-3 until convergence\n",
    "\n",
    "**Questions**:\n",
    "\n",
    "* On which assumptions about data does K-means rely upon?  \n",
    "* What is algorithm complexity?  \n",
    "* Any ideas on good initialization?  \n",
    "* How to select number of clusters k?\n",
    "\n",
    "**Other versions**:\n",
    "\n",
    "* K-means++ - select cluster centers iteratively, with probability to choose an object as a new cluster center  $$p(x_j = c_{k+1}) \\sim \\min_{k} d^2(x_, c_k)$$ \n",
    "* K-medoids - for non-vector space. For $c_k$ is always selected some object $x_i$ from the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b5875b",
   "metadata": {},
   "source": [
    "### 1.3 DBSCAN\n",
    "\n",
    "density-based clustering with noise. \n",
    "\n",
    "**Algorithm sketch**:  \n",
    "\n",
    "1. For each sample $x_i$ find neighbors in the sphere of radius $\\epsilon$ so \n",
    "$$N(x) = \\{x_j | d(x, x_i) \\leq \\epsilon \\}$$  \n",
    "1. Define core points as samples with more then a *min_pts* of neightbors within $\\epsilon$-neighborhood. \n",
    "1. Find connected components of core points on the neighborhood graph, set them as clusters.  \n",
    "1. Assign all non-core points to the nearest cluster if it is within $\\epsilon$-neighborhood\n",
    "1. All non-clustered points are noise.\n",
    "\n",
    "\n",
    "Questions:  \n",
    "\n",
    "1. Pros and cons of DBSCAN vs Kmeans? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe53db37",
   "metadata": {},
   "source": [
    "### 1.4 Hierarchical Clustering \n",
    "\n",
    "Clustering is perfomed from bottom to top.  \n",
    "\n",
    "\n",
    "\n",
    "**Algorithm sketch**:  \n",
    "1. Assign each object a its own cluster. \n",
    "1. Merge 2 closest clusters according to some linkage criteria until there is only 1 cluster left.  \n",
    "1. Look at dendrogramm and select appropriate clustering.  \n",
    "\n",
    "\n",
    "**Linkage criteria**:\n",
    "\n",
    "distance between clusters A and B.\n",
    "\n",
    "* Complete Linkage\n",
    "\n",
    "$$\\max_{a \\in A, b \\in B} d(a,b)$$\n",
    "\n",
    "* Single Linkage\n",
    "\n",
    "$$\\min_{a \\in A, b \\in B} d(a,b)$$\n",
    "\n",
    "* Average linkage\n",
    "\n",
    "$$\\frac 1 {|A||B|} \\sum_{a \\in A, b \\in B} d(a,b)$$\n",
    "\n",
    "* Ward linkage\n",
    "\n",
    "$$ D_{in}(A + B) - D_{in}(A) - D_{in}(B) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e08ec0e",
   "metadata": {},
   "source": [
    "## 2 Dimension Reduction\n",
    "\n",
    "Given a set of points $X \\in R^{NxD}$, find a map into a lower dimension subspace $f: R^D \\rightarrow R^d$ so $d  << D$\n",
    "\n",
    "Depending of the map function there are:  \n",
    "\n",
    "1. Linear methods: PCA, SVD, NMF, etc..\n",
    "1. Nonlinear methods\n",
    "\n",
    "Sometimes, feature selection is considered as dimension reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65cb9a7",
   "metadata": {},
   "source": [
    "### 2.1 Random Projections\n",
    "\n",
    "<img src=\"images/proj.png\" style=\"height:300px\">\n",
    "\n",
    "$$ f(X) = X S $$\n",
    "where $ S \\in R^{Dxd}$\n",
    "\n",
    "A common example is a gaussian random projection:\n",
    "1. $S$ is sampled from a normal gaussian distribution\n",
    "1. each row has unit norm $ ||S_{i,:} || = 1$\n",
    "1. rows are pairwise orthogonal $ S_{i,:}^T S_{j,:} = 0 $ iff $i \\neq j$\n",
    "\n",
    "\n",
    "\n",
    "**Johnson - Lindenstrauss Lemma**:  \n",
    "\n",
    "States, that a set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved.  \n",
    "\n",
    "Given $0 < \\epsilon  < 1$, a set of points $X \\in R^D$, number $d > \\frac {8 \\log(m)} {\\epsilon^2}$, there exists a linear map $f: R^D \\rightarrow R^d$ such that:  \n",
    "$$ (1-\\epsilon) ||u - v||^2 \\leq ||f(u) - f(v)||^2 \\leq (1+\\epsilon) ||u - v||^2$$\n",
    "\n",
    "There exists a set of points of size $m$ that needs dimension $O ( \\frac {\\log(m)} {\\epsilon^2} )$ in order to preserve the distances between all pair of points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37466aa9",
   "metadata": {},
   "source": [
    "### 2.2 PCA\n",
    "Principal Component Analysis  \n",
    "\n",
    "Preserve more information while reducing feature dimensions. Here we make assumption that variance is our measure of information quantity. More variable the feature is, more information it contains.\n",
    "\n",
    "**Algorithm sketch (naive)**:\n",
    "\n",
    "1. center data $\\bar X = X - mean(X)$  \n",
    "1. build covariance matrix $S = \\bar X \\bar X^T$  \n",
    "1. make low-rank approximation for covariance matrix $S \\approx \\hat S_k = U_k \\Sigma_k U_k^T$ by eigendecomposition\n",
    "1. sort eigenvalues in descending order by their absolute value\n",
    "1. select top-k eigenvalues and project data onto corresponding eigenvectors\n",
    "\n",
    "**Learned mapping**:\n",
    "$$f(X) = X U_k$$\n",
    "\n",
    "**Algorithm sketch (practical)**:\n",
    "\n",
    "1. center data $\\bar X = X - mean(X)$  \n",
    "1. find 1 principal component\n",
    "$$\\max_{w_1} ||Xw_1||^2$$\n",
    "$$ ||w_1|| = 1 $$\n",
    "\n",
    "2. find k-th principal component\n",
    "$$\\max_{w_k} ||Xw_k||^2$$\n",
    "$$ ||w_k|| = 1 $$\n",
    "$$ \\forall i \\in 1.. (k-1) ~~~ w_k^T w_i = 0  $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Properties**:\n",
    "1. PCA basis vectors creates an uncorrelated orthogonal basis set.  \n",
    "1. PCA is sensitive to feature scaling \n",
    "\n",
    "<img src=\"images/pca.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235df0d",
   "metadata": {},
   "source": [
    "### 2.3 Truncated SVD\n",
    "\n",
    "**Learning objective**:  \n",
    "$$ || X - \\hat X_k||_F \\rightarrow min$$\n",
    "subject to $rank(X_k) = k$.  \n",
    "\n",
    "which has the exact solution given by SVD decomposition\n",
    "$$ X \\approx \\hat X_k = U_k \\Sigma_k V^T$$\n",
    "\n",
    "**Learned mapping**:   \n",
    "\n",
    "$ \\hat X = X V_k$\n",
    "\n",
    "**Properties**:\n",
    "1. Do not use covariance matrix\n",
    "1. Do not center data\n",
    "1. Preferable over PCA for sparse features\n",
    "\n",
    "<img src=\"images/svd.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15925719",
   "metadata": {},
   "source": [
    "### 2.4 NMF\n",
    "\n",
    "Non-negative matrix Factorization\n",
    "\n",
    "**Learnable objective**:\n",
    "\n",
    "$$ || X - WH ||_F \\rightarrow \\min_{W, H}$$\n",
    "subject to $W \\geq 0$ and $H \\geq 0$.\n",
    "\n",
    "$$ X \\approx \\hat X_k = W H $$\n",
    "\n",
    "Regularized with ElasticNet:  \n",
    "$$ || X - WH ||_F + \\gamma *( \\alpha(||W||_1 + ||H||_1)  + \\frac {1 - \\alpha} 2 ( ||W||_2  + ||H||_2)) \\rightarrow \\min_{W, H} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a004a3e7",
   "metadata": {},
   "source": [
    "## 2.5 Manifold Learning\n",
    "\n",
    "Data lies on a low-dimensional non-linear manifold in a high-dimensional space.  \n",
    "In other words, features are connected by some non-linear function.\n",
    "\n",
    "\n",
    "<img src=\"images/manifold1.png\" style=\"height:300px\">\n",
    "\n",
    "<img src=\"images/manifold2.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86dbb1",
   "metadata": {},
   "source": [
    "### 2.6 AutoEncoders\n",
    "Autoencoders are feed-forward neural networks, which contains encoder and decoder part. All training methods applicable to feed-forward NN, also can be applied to autoencoders. But unlike feed-forward NN, autoencoders are unsipervised models.\n",
    "\n",
    "Let $E$ be encoder,  \n",
    "$D$ - decoder,  \n",
    "$L$  - some loss function.\n",
    "\n",
    "Than, in the most simple case, the goal is to learn an efficient feature represetation by directing information through a bottleneck and predicting sample itself: \n",
    "$$ L(X, D(E(X))) \\rightarrow \\min_{D, E}$$\n",
    "\n",
    "\n",
    "**Denoising Autoencoders**\n",
    "\n",
    "$$ \\bar X = X + \\epsilon $$,\n",
    "usually we use white noise $\\epsilon \\sim N(0,\\sigma)$\n",
    "\n",
    "$$ L(X, D(E(\\bar X))) \\rightarrow \\min_{D, E}$$\n",
    "\n",
    "<img src=\"images/autoencoder.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725dd175",
   "metadata": {},
   "source": [
    "## 3 Neighborhood Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f827a5",
   "metadata": {},
   "source": [
    "## 3.1 t-SNE\n",
    "\n",
    "t-distributed stochastic neighbor embedding\n",
    "\n",
    "**Algorithm sketch**:  \n",
    "1. Compute probability that a sample $x_i$ would peak $x_j$ as its neighbour  \n",
    "$$ p(j | i) = \\frac {1} {Z} \\exp (- \\frac {|| x_i - x_j||_2^2} { 2 \\sigma_i^2}) $$  \n",
    "$Z = \\sum_{k \\neq i} \\exp (- \\frac {|| x_i - x_k||_2^2} { 2 \\sigma_i^2}) $ - normalization factor\n",
    "\n",
    "2. probability that $x_i$ and $x_j$ are neighbors:  \n",
    "$$ p(i,j) = \\frac {p(i | j)  + p(j | i)} {2N}$$  \n",
    "but $p(i,i) = 0$\n",
    "\n",
    "3. introduce map $Y \\in R^d$, for wich probability that $y_i$ and $y_j$ are neighbors:   \n",
    "$$ q(i, j) = \\frac 1 Z (1 + || y_i - y_j||^2 )^{-1}$$\n",
    "$Z = \\sum_{k \\neq i} (1 + || y_i - y_k||^2 )^{-1} $ - normalization factor\n",
    "\n",
    "4. **Learning objective:**  \n",
    "minimize Kullback–Leibler divergence between distributions:\n",
    "$$ KL(P || Q) = \\sum_{i \\neq j} p(i,j) \\log \\frac {p(i,j)} {q(i,j)} \\rightarrow \\min_{Y} $$\n",
    "\n",
    "\n",
    "Questions:\n",
    "* Why is this paricular form of $q(i,j)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08108da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
